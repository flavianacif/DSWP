{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfC0H9bQhXs3GVfc74+5w0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flavianacif/DSWP/blob/master/RFB_RN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0_YZ6IURZE_"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZX00UN5cjvM"
      },
      "source": [
        "[**Python**] - Verificar a versão do Tensorflow\n",
        "> Assegurar que está a utilizar a versão 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THWNIk_FCe_g",
        "outputId": "1878ee27-5728-4b96-c118-be996ac9122b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZgQAKqLcLX3"
      },
      "source": [
        "[**Python**] - Definir o número de casas decimais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzKor02BCe_d"
      },
      "source": [
        "np.set_printoptions(precision= 3)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5V4KopjLWOL"
      },
      "source": [
        "### 1. Carregar os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_cwAUW3tseE"
      },
      "source": [
        "[**Python**] - Carregar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bs87IWPtwtm"
      },
      "source": [
        "# Leitura do dataframe:\n",
        "df_train = pd.read_csv('/train_2.csv')"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URxegHdQUD5P",
        "outputId": "18303d0a-4954-4194-c7de-259278fbf47d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11033, 63)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBUeMtV7tzw6"
      },
      "source": [
        "[**Python**] - Mostrar as primeiras 5 linhas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcH-y4amt3gs",
        "outputId": "8c009f6d-7e0a-4254-8bc0-e9d32a3c3344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cnae2</th>\n",
              "      <th>rf2</th>\n",
              "      <th>md1</th>\n",
              "      <th>md2</th>\n",
              "      <th>md3</th>\n",
              "      <th>md4</th>\n",
              "      <th>md5</th>\n",
              "      <th>md6</th>\n",
              "      <th>md7</th>\n",
              "      <th>md8</th>\n",
              "      <th>md9</th>\n",
              "      <th>md10</th>\n",
              "      <th>md11</th>\n",
              "      <th>md12</th>\n",
              "      <th>mc1</th>\n",
              "      <th>mc2</th>\n",
              "      <th>mc3</th>\n",
              "      <th>mc4</th>\n",
              "      <th>ind01</th>\n",
              "      <th>ind02</th>\n",
              "      <th>ind03</th>\n",
              "      <th>ind04</th>\n",
              "      <th>ind05</th>\n",
              "      <th>ind06</th>\n",
              "      <th>ind07</th>\n",
              "      <th>ind08</th>\n",
              "      <th>ind09</th>\n",
              "      <th>ind10</th>\n",
              "      <th>ind11</th>\n",
              "      <th>ind12</th>\n",
              "      <th>ind13</th>\n",
              "      <th>ind14</th>\n",
              "      <th>ind15</th>\n",
              "      <th>ind16</th>\n",
              "      <th>ind17</th>\n",
              "      <th>ind18</th>\n",
              "      <th>ind19</th>\n",
              "      <th>ind20</th>\n",
              "      <th>ind21</th>\n",
              "      <th>ind22</th>\n",
              "      <th>ind23</th>\n",
              "      <th>ind24</th>\n",
              "      <th>ind25</th>\n",
              "      <th>ind26</th>\n",
              "      <th>ind27</th>\n",
              "      <th>ind28</th>\n",
              "      <th>ind29</th>\n",
              "      <th>ind30</th>\n",
              "      <th>ind31</th>\n",
              "      <th>ind32</th>\n",
              "      <th>ind33</th>\n",
              "      <th>ind34</th>\n",
              "      <th>ind35</th>\n",
              "      <th>ind36</th>\n",
              "      <th>ind37</th>\n",
              "      <th>ind38</th>\n",
              "      <th>ind39</th>\n",
              "      <th>ind40</th>\n",
              "      <th>ind41</th>\n",
              "      <th>ind42</th>\n",
              "      <th>ind43</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015101</td>\n",
              "      <td>0.011256</td>\n",
              "      <td>0.111095</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016366</td>\n",
              "      <td>0.021082</td>\n",
              "      <td>0.004541</td>\n",
              "      <td>0.004541</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.130930</td>\n",
              "      <td>0.003945</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0281</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0976</td>\n",
              "      <td>0.0333</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>0.005996</td>\n",
              "      <td>0.019476</td>\n",
              "      <td>0.124770</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010487</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.029214</td>\n",
              "      <td>0.046445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018198</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.138620</td>\n",
              "      <td>0.003186</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4167</td>\n",
              "      <td>0.4194</td>\n",
              "      <td>0.7068</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7625</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.1429</td>\n",
              "      <td>0.2857</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.4444</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>74</td>\n",
              "      <td>9</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.002902</td>\n",
              "      <td>0.110160</td>\n",
              "      <td>0.002274</td>\n",
              "      <td>0.002274</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002902</td>\n",
              "      <td>0.020058</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>49</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.014526</td>\n",
              "      <td>0.120351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.014526</td>\n",
              "      <td>0.032017</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.138620</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2309</td>\n",
              "      <td>0.2309</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.1785</td>\n",
              "      <td>0.1667</td>\n",
              "      <td>0.1667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.004042</td>\n",
              "      <td>0.111078</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006063</td>\n",
              "      <td>0.023705</td>\n",
              "      <td>0.011886</td>\n",
              "      <td>0.011886</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131285</td>\n",
              "      <td>0.001925</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  cnae2  rf2       md1       md2  ...  ind40  ind41  ind42  ind43  target\n",
              "0   0     86    1  0.015101  0.011256  ...    0.0    0.0    0.0    0.0    True\n",
              "1   1     18    9  0.005996  0.019476  ...    0.0    0.0    0.0    0.0   False\n",
              "2   2     74    9  0.000006  0.002902  ...    0.0    0.0    0.0    0.0   False\n",
              "3   3     49    4  0.000009  0.014526  ...    0.0    0.0    0.0    0.0   False\n",
              "4   4     47    1  0.000191  0.004042  ...    0.0    0.0    0.0    0.0   False\n",
              "\n",
              "[5 rows x 63 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSa161sPLcAw"
      },
      "source": [
        "### Pré-processamento e transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL2-6wpCuARF"
      },
      "source": [
        "[**Python**] - Construir coluna 'sexo' da seguinte forma:\n",
        "* Se Gender= 'Male' ==> sexo= 1;\n",
        "* Se Gender= 'Female' ==> sexo= 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccImSqCqDKre"
      },
      "source": [
        "def define_label(row):\n",
        "    if row['Gender'] == 'Male':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDYamauZCq77"
      },
      "source": [
        "df_sexo['sexo'] = df_sexo.apply(lambda row: define_label(row), axis = 1)\n",
        "df_sexo.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqkOrJnNuZjg"
      },
      "source": [
        "[**Python**] - Renomear ou reescrever os nomes das colunas do dataframe em letras minúsculas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dahUMI6DsBz"
      },
      "source": [
        "df_sexo = df_sexo.drop(columns= 'Gender', axis= 1)\n",
        "df_sexo = df_sexo.rename({'Height': 'altura', 'Weight': 'peso'}, axis= 1)\n",
        "df_sexo.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTISVuZ4ukQO"
      },
      "source": [
        "[**Python**] - Definir os arrays df_X e df_y:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMTIn6Zf5LlU"
      },
      "source": [
        "df_X = df_train.copy()\n",
        "df_X = df_X.drop(columns= ['target','id'])\n",
        "df_y = df_train['target'].values"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSThKwhj4LsC",
        "outputId": "2f5d44ae-b127-4145-873a-808fa8a8299f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_y"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False, False, ...,  True, False,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiO_F95jc1_s"
      },
      "source": [
        "[**Python**] - Normalizar os dados - StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOfIzZWbS4oP",
        "outputId": "4da1f373-7e8c-41e9-cd3c-876e27624e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "col_num = df_X.select_dtypes(include=['float64']).columns\n",
        "col_num"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['md1', 'md2', 'md3', 'md4', 'md5', 'md6', 'md7', 'md8', 'md9', 'md10',\n",
              "       'md11', 'md12', 'mc1', 'mc2', 'mc3', 'mc4', 'ind01', 'ind02', 'ind03',\n",
              "       'ind04', 'ind05', 'ind06', 'ind07', 'ind08', 'ind09', 'ind10', 'ind11',\n",
              "       'ind12', 'ind13', 'ind14', 'ind15', 'ind16', 'ind17', 'ind18', 'ind19',\n",
              "       'ind20', 'ind21', 'ind22', 'ind23', 'ind24', 'ind25', 'ind26', 'ind27',\n",
              "       'ind28', 'ind29', 'ind30', 'ind31', 'ind32', 'ind33', 'ind34', 'ind35',\n",
              "       'ind36', 'ind37', 'ind38', 'ind39', 'ind40', 'ind41', 'ind42', 'ind43'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4myPAnSzE7-l",
        "outputId": "4cd51ddc-494c-456e-eada-1147d328adae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SS = StandardScaler()\n",
        "\n",
        "df_X[col_num]= SS.fit_transform(df_X[col_num])\n",
        "df_X"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnae2</th>\n",
              "      <th>rf2</th>\n",
              "      <th>md1</th>\n",
              "      <th>md2</th>\n",
              "      <th>md3</th>\n",
              "      <th>md4</th>\n",
              "      <th>md5</th>\n",
              "      <th>md6</th>\n",
              "      <th>md7</th>\n",
              "      <th>md8</th>\n",
              "      <th>md9</th>\n",
              "      <th>md10</th>\n",
              "      <th>md11</th>\n",
              "      <th>md12</th>\n",
              "      <th>mc1</th>\n",
              "      <th>mc2</th>\n",
              "      <th>mc3</th>\n",
              "      <th>mc4</th>\n",
              "      <th>ind01</th>\n",
              "      <th>ind02</th>\n",
              "      <th>ind03</th>\n",
              "      <th>ind04</th>\n",
              "      <th>ind05</th>\n",
              "      <th>ind06</th>\n",
              "      <th>ind07</th>\n",
              "      <th>ind08</th>\n",
              "      <th>ind09</th>\n",
              "      <th>ind10</th>\n",
              "      <th>ind11</th>\n",
              "      <th>ind12</th>\n",
              "      <th>ind13</th>\n",
              "      <th>ind14</th>\n",
              "      <th>ind15</th>\n",
              "      <th>ind16</th>\n",
              "      <th>ind17</th>\n",
              "      <th>ind18</th>\n",
              "      <th>ind19</th>\n",
              "      <th>ind20</th>\n",
              "      <th>ind21</th>\n",
              "      <th>ind22</th>\n",
              "      <th>ind23</th>\n",
              "      <th>ind24</th>\n",
              "      <th>ind25</th>\n",
              "      <th>ind26</th>\n",
              "      <th>ind27</th>\n",
              "      <th>ind28</th>\n",
              "      <th>ind29</th>\n",
              "      <th>ind30</th>\n",
              "      <th>ind31</th>\n",
              "      <th>ind32</th>\n",
              "      <th>ind33</th>\n",
              "      <th>ind34</th>\n",
              "      <th>ind35</th>\n",
              "      <th>ind36</th>\n",
              "      <th>ind37</th>\n",
              "      <th>ind38</th>\n",
              "      <th>ind39</th>\n",
              "      <th>ind40</th>\n",
              "      <th>ind41</th>\n",
              "      <th>ind42</th>\n",
              "      <th>ind43</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86</td>\n",
              "      <td>1</td>\n",
              "      <td>1.890364</td>\n",
              "      <td>-0.063053</td>\n",
              "      <td>-0.330943</td>\n",
              "      <td>-0.272439</td>\n",
              "      <td>-0.164627</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>0.023468</td>\n",
              "      <td>-0.406553</td>\n",
              "      <td>-0.300598</td>\n",
              "      <td>-0.180130</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.254990</td>\n",
              "      <td>0.554777</td>\n",
              "      <td>0.009963</td>\n",
              "      <td>-0.185666</td>\n",
              "      <td>0.376544</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.409368</td>\n",
              "      <td>-0.735539</td>\n",
              "      <td>-0.806095</td>\n",
              "      <td>-0.842409</td>\n",
              "      <td>-0.839243</td>\n",
              "      <td>-0.707050</td>\n",
              "      <td>-0.804816</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.527262</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>0.673623</td>\n",
              "      <td>-1.556934</td>\n",
              "      <td>-1.247419</td>\n",
              "      <td>-1.215785</td>\n",
              "      <td>-0.764612</td>\n",
              "      <td>-0.751150</td>\n",
              "      <td>0.014547</td>\n",
              "      <td>-1.303257</td>\n",
              "      <td>-1.119314</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>-0.701970</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>-0.975378</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>0.607337</td>\n",
              "      <td>0.716694</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>-0.136319</td>\n",
              "      <td>0.246991</td>\n",
              "      <td>0.300066</td>\n",
              "      <td>-0.354571</td>\n",
              "      <td>0.799216</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>0.466149</td>\n",
              "      <td>0.521429</td>\n",
              "      <td>-0.410355</td>\n",
              "      <td>1.015943</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>1.067028</td>\n",
              "      <td>0.071244</td>\n",
              "      <td>0.494039</td>\n",
              "      <td>0.325950</td>\n",
              "      <td>0.259066</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.498650</td>\n",
              "      <td>1.392895</td>\n",
              "      <td>0.117190</td>\n",
              "      <td>0.092460</td>\n",
              "      <td>0.655920</td>\n",
              "      <td>-0.689384</td>\n",
              "      <td>-0.787562</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.177127</td>\n",
              "      <td>-0.438572</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>-1.540288</td>\n",
              "      <td>0.136321</td>\n",
              "      <td>0.996572</td>\n",
              "      <td>1.033080</td>\n",
              "      <td>-0.429099</td>\n",
              "      <td>-0.080470</td>\n",
              "      <td>-0.435213</td>\n",
              "      <td>-0.309639</td>\n",
              "      <td>-0.815049</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>1.424563</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>1.025244</td>\n",
              "      <td>-3.204631</td>\n",
              "      <td>0.607337</td>\n",
              "      <td>-1.395295</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>74</td>\n",
              "      <td>9</td>\n",
              "      <td>-0.280270</td>\n",
              "      <td>-0.378131</td>\n",
              "      <td>-0.374080</td>\n",
              "      <td>-0.296796</td>\n",
              "      <td>-0.191079</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>-0.440452</td>\n",
              "      <td>-0.444037</td>\n",
              "      <td>-0.334673</td>\n",
              "      <td>-0.217285</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.291850</td>\n",
              "      <td>-0.181695</td>\n",
              "      <td>-0.130871</td>\n",
              "      <td>-0.192310</td>\n",
              "      <td>-0.222879</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.498650</td>\n",
              "      <td>1.392895</td>\n",
              "      <td>1.409612</td>\n",
              "      <td>1.386654</td>\n",
              "      <td>1.276154</td>\n",
              "      <td>-0.707050</td>\n",
              "      <td>-0.804816</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.527262</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>0.673623</td>\n",
              "      <td>0.663728</td>\n",
              "      <td>0.996572</td>\n",
              "      <td>1.033080</td>\n",
              "      <td>1.583274</td>\n",
              "      <td>1.596347</td>\n",
              "      <td>-0.435213</td>\n",
              "      <td>1.033231</td>\n",
              "      <td>1.121589</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>1.424563</td>\n",
              "      <td>1.315395</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>1.025244</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>0.607337</td>\n",
              "      <td>0.716694</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.280199</td>\n",
              "      <td>0.060275</td>\n",
              "      <td>0.096169</td>\n",
              "      <td>-0.354571</td>\n",
              "      <td>-0.253826</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>-0.039954</td>\n",
              "      <td>-0.006465</td>\n",
              "      <td>-0.410355</td>\n",
              "      <td>-0.299810</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>0.562924</td>\n",
              "      <td>-0.147933</td>\n",
              "      <td>-0.130871</td>\n",
              "      <td>-0.192878</td>\n",
              "      <td>-0.236365</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.498650</td>\n",
              "      <td>-0.735539</td>\n",
              "      <td>-0.806095</td>\n",
              "      <td>-0.842409</td>\n",
              "      <td>-0.839243</td>\n",
              "      <td>-0.170321</td>\n",
              "      <td>-0.280608</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.527262</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>-1.540288</td>\n",
              "      <td>0.663728</td>\n",
              "      <td>-0.686421</td>\n",
              "      <td>-0.653569</td>\n",
              "      <td>1.387696</td>\n",
              "      <td>1.400801</td>\n",
              "      <td>3.185443</td>\n",
              "      <td>-0.980833</td>\n",
              "      <td>-0.953238</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>-0.701970</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>-0.975378</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>-1.646532</td>\n",
              "      <td>0.716694</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.275820</td>\n",
              "      <td>-0.335136</td>\n",
              "      <td>-0.331722</td>\n",
              "      <td>-0.326093</td>\n",
              "      <td>-0.222897</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>-0.331538</td>\n",
              "      <td>-0.310604</td>\n",
              "      <td>-0.123057</td>\n",
              "      <td>0.013463</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.230032</td>\n",
              "      <td>-0.029356</td>\n",
              "      <td>0.262544</td>\n",
              "      <td>0.281748</td>\n",
              "      <td>-0.150195</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.498650</td>\n",
              "      <td>-0.735539</td>\n",
              "      <td>-0.806095</td>\n",
              "      <td>-0.842409</td>\n",
              "      <td>-0.839243</td>\n",
              "      <td>1.617460</td>\n",
              "      <td>1.465467</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.527262</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>-1.540288</td>\n",
              "      <td>-1.556934</td>\n",
              "      <td>-1.247419</td>\n",
              "      <td>-1.215785</td>\n",
              "      <td>-0.764612</td>\n",
              "      <td>-0.751150</td>\n",
              "      <td>-0.435213</td>\n",
              "      <td>-1.383742</td>\n",
              "      <td>-1.368303</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>-0.701970</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>-0.975378</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>-1.646532</td>\n",
              "      <td>-1.395295</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>2.623112</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>2.279218</td>\n",
              "      <td>2.048480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11028</th>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.280409</td>\n",
              "      <td>-0.442027</td>\n",
              "      <td>-0.404083</td>\n",
              "      <td>-0.354571</td>\n",
              "      <td>-0.233705</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>-0.478011</td>\n",
              "      <td>-0.463384</td>\n",
              "      <td>-0.410355</td>\n",
              "      <td>-0.259050</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.260446</td>\n",
              "      <td>-0.152399</td>\n",
              "      <td>-0.130871</td>\n",
              "      <td>-0.169328</td>\n",
              "      <td>-0.194070</td>\n",
              "      <td>2.925127</td>\n",
              "      <td>2.678633</td>\n",
              "      <td>1.392895</td>\n",
              "      <td>1.409612</td>\n",
              "      <td>1.386654</td>\n",
              "      <td>-0.839243</td>\n",
              "      <td>-0.707050</td>\n",
              "      <td>-0.804816</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.127535</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>0.673623</td>\n",
              "      <td>-1.556934</td>\n",
              "      <td>0.996572</td>\n",
              "      <td>1.033080</td>\n",
              "      <td>1.583274</td>\n",
              "      <td>1.596347</td>\n",
              "      <td>-0.435213</td>\n",
              "      <td>1.033231</td>\n",
              "      <td>-0.953238</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>1.424563</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>1.025244</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>0.607337</td>\n",
              "      <td>-1.395295</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11029</th>\n",
              "      <td>49</td>\n",
              "      <td>7</td>\n",
              "      <td>-0.280340</td>\n",
              "      <td>0.145305</td>\n",
              "      <td>0.189024</td>\n",
              "      <td>-0.201334</td>\n",
              "      <td>-0.087403</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>0.037724</td>\n",
              "      <td>0.074557</td>\n",
              "      <td>-0.112301</td>\n",
              "      <td>0.025191</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>0.473726</td>\n",
              "      <td>-0.104082</td>\n",
              "      <td>-0.130871</td>\n",
              "      <td>-0.118679</td>\n",
              "      <td>0.068519</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.498650</td>\n",
              "      <td>-0.735539</td>\n",
              "      <td>-0.806095</td>\n",
              "      <td>-0.842409</td>\n",
              "      <td>-0.839243</td>\n",
              "      <td>-0.707050</td>\n",
              "      <td>-0.804816</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.527262</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>-1.540288</td>\n",
              "      <td>-1.556934</td>\n",
              "      <td>0.622499</td>\n",
              "      <td>-0.091353</td>\n",
              "      <td>-0.764612</td>\n",
              "      <td>-0.751150</td>\n",
              "      <td>0.140811</td>\n",
              "      <td>0.831897</td>\n",
              "      <td>-0.953238</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>-0.701970</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>-0.975378</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>-1.646532</td>\n",
              "      <td>-1.395295</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11030</th>\n",
              "      <td>47</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.280304</td>\n",
              "      <td>-0.443128</td>\n",
              "      <td>-0.404805</td>\n",
              "      <td>-0.354571</td>\n",
              "      <td>-0.253826</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>-0.487697</td>\n",
              "      <td>-0.473487</td>\n",
              "      <td>-0.410355</td>\n",
              "      <td>-0.299810</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.266291</td>\n",
              "      <td>-0.159322</td>\n",
              "      <td>-0.130871</td>\n",
              "      <td>-0.165414</td>\n",
              "      <td>-0.196862</td>\n",
              "      <td>2.925127</td>\n",
              "      <td>2.678633</td>\n",
              "      <td>1.392895</td>\n",
              "      <td>1.409612</td>\n",
              "      <td>1.386654</td>\n",
              "      <td>1.276154</td>\n",
              "      <td>-0.707050</td>\n",
              "      <td>-0.804816</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.527262</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>0.673623</td>\n",
              "      <td>0.663728</td>\n",
              "      <td>0.996572</td>\n",
              "      <td>1.033080</td>\n",
              "      <td>1.583274</td>\n",
              "      <td>1.596347</td>\n",
              "      <td>-0.435213</td>\n",
              "      <td>1.033231</td>\n",
              "      <td>1.121589</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>1.424563</td>\n",
              "      <td>1.315395</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>1.025244</td>\n",
              "      <td>-3.204631</td>\n",
              "      <td>0.607337</td>\n",
              "      <td>-1.395295</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11031</th>\n",
              "      <td>47</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.279898</td>\n",
              "      <td>-0.418890</td>\n",
              "      <td>-0.388924</td>\n",
              "      <td>-0.290447</td>\n",
              "      <td>-0.184183</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>-0.070142</td>\n",
              "      <td>-0.037953</td>\n",
              "      <td>0.042872</td>\n",
              "      <td>0.194404</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.255964</td>\n",
              "      <td>0.215739</td>\n",
              "      <td>0.209659</td>\n",
              "      <td>1.044450</td>\n",
              "      <td>0.112028</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.498650</td>\n",
              "      <td>-0.735539</td>\n",
              "      <td>-0.806095</td>\n",
              "      <td>-0.842409</td>\n",
              "      <td>-0.839243</td>\n",
              "      <td>-0.707050</td>\n",
              "      <td>-0.804816</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>-0.451581</td>\n",
              "      <td>-0.527262</td>\n",
              "      <td>0.233394</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>0.673623</td>\n",
              "      <td>0.663728</td>\n",
              "      <td>-0.686421</td>\n",
              "      <td>-0.653569</td>\n",
              "      <td>-0.764612</td>\n",
              "      <td>-0.751150</td>\n",
              "      <td>1.100697</td>\n",
              "      <td>-1.182408</td>\n",
              "      <td>-1.160895</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>-0.701970</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>-0.975378</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>-1.646532</td>\n",
              "      <td>-1.395295</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11032</th>\n",
              "      <td>47</td>\n",
              "      <td>8</td>\n",
              "      <td>-0.279576</td>\n",
              "      <td>-0.348698</td>\n",
              "      <td>-0.342851</td>\n",
              "      <td>-0.271461</td>\n",
              "      <td>-0.164404</td>\n",
              "      <td>-0.08485</td>\n",
              "      <td>0.409453</td>\n",
              "      <td>0.462292</td>\n",
              "      <td>0.424628</td>\n",
              "      <td>0.603831</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.112997</td>\n",
              "      <td>0.102963</td>\n",
              "      <td>0.658051</td>\n",
              "      <td>0.693508</td>\n",
              "      <td>-0.044748</td>\n",
              "      <td>-0.348266</td>\n",
              "      <td>-0.498650</td>\n",
              "      <td>-0.735539</td>\n",
              "      <td>-0.806095</td>\n",
              "      <td>-0.842409</td>\n",
              "      <td>-0.839243</td>\n",
              "      <td>-0.707050</td>\n",
              "      <td>-0.804816</td>\n",
              "      <td>-0.107761</td>\n",
              "      <td>-0.04317</td>\n",
              "      <td>-0.037289</td>\n",
              "      <td>0.772241</td>\n",
              "      <td>1.509827</td>\n",
              "      <td>-0.214115</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>-0.063268</td>\n",
              "      <td>-0.124995</td>\n",
              "      <td>-0.064549</td>\n",
              "      <td>-1.540288</td>\n",
              "      <td>-1.556934</td>\n",
              "      <td>-1.247419</td>\n",
              "      <td>-1.215785</td>\n",
              "      <td>1.387696</td>\n",
              "      <td>-0.359822</td>\n",
              "      <td>2.099292</td>\n",
              "      <td>-1.182408</td>\n",
              "      <td>-1.368303</td>\n",
              "      <td>-0.085108</td>\n",
              "      <td>-0.038109</td>\n",
              "      <td>-0.022188</td>\n",
              "      <td>-0.701970</td>\n",
              "      <td>-0.760228</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>-0.975378</td>\n",
              "      <td>0.312048</td>\n",
              "      <td>-1.646532</td>\n",
              "      <td>0.716694</td>\n",
              "      <td>-0.069476</td>\n",
              "      <td>-0.381227</td>\n",
              "      <td>-0.016492</td>\n",
              "      <td>-0.009521</td>\n",
              "      <td>-0.438747</td>\n",
              "      <td>-0.425347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11033 rows × 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       cnae2  rf2       md1       md2  ...     ind40     ind41     ind42     ind43\n",
              "0         86    1  1.890364 -0.063053  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "1         18    9 -0.136319  0.246991  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "2         74    9 -0.280270 -0.378131  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "3         49    4 -0.280199  0.060275  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "4         47    1 -0.275820 -0.335136  ... -0.016492 -0.009521  2.279218  2.048480\n",
              "...      ...  ...       ...       ...  ...       ...       ...       ...       ...\n",
              "11028     23    6 -0.280409 -0.442027  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "11029     49    7 -0.280340  0.145305  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "11030     47    8 -0.280304 -0.443128  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "11031     47    3 -0.279898 -0.418890  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "11032     47    8 -0.279576 -0.348698  ... -0.016492 -0.009521 -0.438747 -0.425347\n",
              "\n",
              "[11033 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJaJWuUqJCha"
      },
      "source": [
        "### 3. Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoO2iEimu4SQ"
      },
      "source": [
        "[**Python**] - Definir as amostras de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTCdm-F9JBGA",
        "outputId": "1e9fe813-3f19-4510-8eb2-77052af75b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_treinamento, X_teste, y_treinamento, y_teste= train_test_split(df_X, df_y, test_size = 0.1, random_state = 20111974)\n",
        "print(f'X: Treinamento=  {X_treinamento.shape}; X: Teste=  {X_teste.shape}')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: Treinamento=  (9929, 61); X: Teste=  (1104, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th9CsQpB8VDK",
        "outputId": "5e984c02-12ed-49b0-a65a-0b3c393031be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'Y: Treinamento =  {y_treinamento.shape}; Y: Teste = {y_teste.shape}')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y: Treinamento =  (9929,); Y: Teste = (1104,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bL-vXiULupD"
      },
      "source": [
        "### 4. Definir a arquitetura da Rede Neural com _Tensorflow_/_Keras_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxETX6dTfyU5"
      },
      "source": [
        "[**Python**] - Definir a arquitetura, ou seja:\n",
        "* $N_{I}$: Número de neurônios na camada de entrada (_Input Layer_);\n",
        "* $N_{O}$: Número de neurônios na camada de saída (_Output Layer_);\n",
        "* $N_{H}$: Número de neurônios na camada escondida (_Hidden Layer_);\n",
        "* FA: Função de ativação;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_MdsLicfyU6"
      },
      "source": [
        "# Número de Neurônios na Input Layer:\n",
        "N_I = 61\n",
        "\n",
        "# Número de neurônios na Output Layer:\n",
        "N_O = 2\n",
        "\n",
        "# Número de neurônios na Hidden Layer:\n",
        "N_H = 175\n",
        "\n",
        "N_H2 = 175\n",
        "\n",
        "# Função de Ativação da Hidden Layer:\n",
        "FA_H = tf.keras.activations.swish\n",
        "\n",
        "# Função de Ativação da Output Layer:\n",
        "FA_O = tf.keras.activations.sigmoid\n",
        "\n",
        "\n"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUMmDuPCcYyB"
      },
      "source": [
        "[**Python**] - Definir as sementes para NumPy e Tensorflow:\n",
        "> Por questões de reproducibilidade de resultados, use as sementes abaixo:\n",
        "\n",
        "* NumPy: 20111974;\n",
        "* Tensorflow: 20111974;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-echOBmceVy"
      },
      "source": [
        "np.random.seed(22091980)\n",
        "tf.random.set_seed(19800922)"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZceRRdinEM2"
      },
      "source": [
        "[**Python**] - Definir a Rede Neural:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXQsSYq2DBfI"
      },
      "source": [
        "* 1 camada _dropout_ com $p= 0.1$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFR5Kr_nDtD",
        "outputId": "9adb03cf-4611-4481-8052-d958e0f56402",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "RN= Sequential()\n",
        "RN.add(Dense(N_H, input_dim= N_I, kernel_initializer= tf.keras.initializers.GlorotNormal(), activation= FA_H, kernel_constraint= tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(N_H2, kernel_initializer= tf.keras.initializers.GlorotNormal(), activation= FA_H, kernel_constraint= tf.keras.constraints.UnitNorm()))\n",
        "RN.add(Dropout(0.1))\n",
        "RN.add(Dense(units= N_O, activation= FA_O))\n",
        "\n",
        "# Resumo da arquitetura da Rede Neural\n",
        "print(RN.summary())"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_97 (Dense)             (None, 175)               10850     \n",
            "_________________________________________________________________\n",
            "dropout_64 (Dropout)         (None, 175)               0         \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 175)               30800     \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 175)               0         \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 2)                 352       \n",
            "=================================================================\n",
            "Total params: 42,002\n",
            "Trainable params: 42,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JBZf4ypGO8o"
      },
      "source": [
        "### 5. Compilar a Rede Neural\n",
        "\n",
        "Este é um problema de classificação binária (_Male_ ou _Female_). Portanto, temos:\n",
        "* optimizer= tf.keras.optimizers.Adam();\n",
        "* loss=  tf.keras.losses.MeanSquaredError() ou loss= tf.keras.losses.BinaryCrossentropy(). Particularmente, eu gosto de usar loss=  tf.keras.losses.MeanSquaredError() porque o resultado é mais intuitivo;\n",
        "* metrics= tf.keras.metrics.binary_accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USmAuw6f00wL"
      },
      "source": [
        "[**Python**] - Comando modelo.compile(optimizer, loss, metrics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7KEi1_e6SSF"
      },
      "source": [
        "#Algoritmo_Opt = tf.keras.optimizers.Adam()beta_1=0.8, beta_2=0.999\n",
        "Algoritmo_Opt = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.8, beta_2=0.999, epsilon=1e-07\n",
        "                                        , amsgrad=True,     name='Adam')\n",
        "Loss_Function = tf.keras.losses.MeanSquaredError()\n",
        "Metrics_Perf = tf.keras.metrics.binary_accuracy\n",
        "\n",
        "RN.compile(optimizer = Algoritmo_Opt, loss = Loss_Function, metrics = Metrics_Perf)"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc90EeV_GojX"
      },
      "source": [
        "### 6. Ajustar a Rede Neural\n",
        "\n",
        "Obs.: A opção callbacks abaixo implementa o conceito de _early stopping_. Esta opção vai parar o processo de treinamento da Rede Neural antes de atingirmos o númerco de _epochs_ quando o modelo pára de melhorar, medido pela métrica val_loss. O parâmetro _patience_= k significa que o processo de otimização vai parar se tivermos k _epochs_ consecutivas sem observarmos melhoria da performance da Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCCTtUh_vEFP"
      },
      "source": [
        "[**Python**] - Comando modelo.fit(X_treinamento, y_treinamento, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB91J6nrF0db",
        "outputId": "60d261cc-826c-4430-eda7-890e6402d65b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 25, min_delta = 0.001)]\n",
        "hist= RN.fit(X_treinamento, y_treinamento, epochs = 100, \n",
        "             validation_data = (X_teste, y_teste), \n",
        "             callbacks = callbacks)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2303 - binary_accuracy: 0.7692 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 2/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 3/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2306 - binary_accuracy: 0.7694 - val_loss: 0.2202 - val_binary_accuracy: 0.7799\n",
            "Epoch 4/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2297 - binary_accuracy: 0.7703 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 5/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 6/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 7/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 8/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 9/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 10/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 11/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 12/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 13/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 14/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 15/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 16/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 17/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 18/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 19/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 20/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 21/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 22/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 23/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 24/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 25/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n",
            "Epoch 26/100\n",
            "311/311 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.7706 - val_loss: 0.2192 - val_binary_accuracy: 0.7808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9LNgeg6ci5u"
      },
      "source": [
        "def Model_Loss(hist):\n",
        "    print(hist.history.keys())\n",
        "    plt.plot(hist.history['loss'])\n",
        "    plt.plot(hist.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'], loc= 'upper right')\n",
        "    plt.show()\n",
        "\n",
        "def Model_Accuracy(hist):\n",
        "    print(hist.history.keys())\n",
        "    plt.plot(hist.history['accuracy'])\n",
        "    plt.plot(hist.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'], loc= 'upper right')\n",
        "    plt.show()\n",
        "\n",
        "def Model_MSE(hist):\n",
        "    print(hist.history.keys())\n",
        "    plt.plot(hist.history['mse'])\n",
        "    plt.plot(hist.history['val_mse'])\n",
        "    plt.title('Model MSE')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Training', 'Validation'], loc= 'upper right')\n",
        "    plt.show()\n",
        "\n",
        "def Mostra_ConfusionMatrix():\n",
        "    y_pred = RN.predict_classes(X_teste)\n",
        "    mc = confusion_matrix(y_teste, y_pred)\n",
        "    #sns.heatmap(mc,annot=True, annot_kws={\"size\": 10},fmt=\"d\")\n",
        "    sns.heatmap(mc/np.sum(mc), annot=True, annot_kws={\"size\": 10}, fmt='.2%', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71mX1iwvHMc5",
        "outputId": "e77c94f2-1005-408c-a8e8-fe9af4c6ddf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "Model_Accuracy(hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-52181b2ceb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel_Accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-102-d8e91b2e8510>\u001b[0m in \u001b[0;36mModel_Accuracy\u001b[0;34m(hist)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mModel_Accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-zJ6GIjHbY8",
        "outputId": "53d8490e-7b98-4831-eef3-63fb5e9bbccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "Model_Loss(hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-8a9781df17aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Model_Loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1sL_DTrKmpq"
      },
      "source": [
        "### 7. Avaliar a performance da Rede Neural\n",
        "\n",
        "Para avaliar a a Rede Neural, simplesmente informamos as amostras de teste: X_teste e y_teste. A função evaluate() vai retornar uma lista contendo 2 valores: loss e accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VckQfEFPvMa7"
      },
      "source": [
        "[**Python**] - Comando modelo.evaluate(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUhEiqxfKmpv",
        "outputId": "72920dfa-5f40-46da-e5dd-ca4934558b42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RN.evaluate(X_teste, y_teste)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 974us/step - loss: 0.2192 - binary_accuracy: 0.7808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.21920281648635864, 0.7807971239089966]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB3jYewpdYUQ",
        "outputId": "960511d2-d28c-4079-c57e-81041fe8205c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist2 = RN.fit(df_X, df_y, epochs = 200, callbacks = callbacks)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "333/345 [===========================>..] - ETA: 0s - loss: 0.2290 - binary_accuracy: 0.7709WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2290 - binary_accuracy: 0.7710\n",
            "Epoch 2/200\n",
            "328/345 [===========================>..] - ETA: 0s - loss: 0.2287 - binary_accuracy: 0.7713WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 3/200\n",
            "324/345 [===========================>..] - ETA: 0s - loss: 0.2274 - binary_accuracy: 0.7726WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 4/200\n",
            "320/345 [==========================>...] - ETA: 0s - loss: 0.2278 - binary_accuracy: 0.7722WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 5/200\n",
            "332/345 [===========================>..] - ETA: 0s - loss: 0.2286 - binary_accuracy: 0.7714WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 6/200\n",
            "341/345 [============================>.] - ETA: 0s - loss: 0.2286 - binary_accuracy: 0.7714WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 7/200\n",
            "327/345 [===========================>..] - ETA: 0s - loss: 0.2280 - binary_accuracy: 0.7720WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 8/200\n",
            "328/345 [===========================>..] - ETA: 0s - loss: 0.2279 - binary_accuracy: 0.7721WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 9/200\n",
            "327/345 [===========================>..] - ETA: 0s - loss: 0.2284 - binary_accuracy: 0.7716WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 10/200\n",
            "332/345 [===========================>..] - ETA: 0s - loss: 0.2278 - binary_accuracy: 0.7722WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 11/200\n",
            "326/345 [===========================>..] - ETA: 0s - loss: 0.2293 - binary_accuracy: 0.7707WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 12/200\n",
            "338/345 [============================>.] - ETA: 0s - loss: 0.2288 - binary_accuracy: 0.7712WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 13/200\n",
            "327/345 [===========================>..] - ETA: 0s - loss: 0.2286 - binary_accuracy: 0.7714WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 14/200\n",
            "321/345 [==========================>...] - ETA: 0s - loss: 0.2285 - binary_accuracy: 0.7715WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 15/200\n",
            "338/345 [============================>.] - ETA: 0s - loss: 0.2292 - binary_accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 16/200\n",
            "321/345 [==========================>...] - ETA: 0s - loss: 0.2299 - binary_accuracy: 0.7701WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 17/200\n",
            "329/345 [===========================>..] - ETA: 0s - loss: 0.2270 - binary_accuracy: 0.7730WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 18/200\n",
            "343/345 [============================>.] - ETA: 0s - loss: 0.2286 - binary_accuracy: 0.7714WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 19/200\n",
            "325/345 [===========================>..] - ETA: 0s - loss: 0.2284 - binary_accuracy: 0.7716WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 20/200\n",
            "332/345 [===========================>..] - ETA: 0s - loss: 0.2278 - binary_accuracy: 0.7722WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 21/200\n",
            "328/345 [===========================>..] - ETA: 0s - loss: 0.2283 - binary_accuracy: 0.7717WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 22/200\n",
            "344/345 [============================>.] - ETA: 0s - loss: 0.2280 - binary_accuracy: 0.7720WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 23/200\n",
            "332/345 [===========================>..] - ETA: 0s - loss: 0.2289 - binary_accuracy: 0.7711WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 24/200\n",
            "331/345 [===========================>..] - ETA: 0s - loss: 0.2287 - binary_accuracy: 0.7713WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 25/200\n",
            "320/345 [==========================>...] - ETA: 0s - loss: 0.2288 - binary_accuracy: 0.7712WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 26/200\n",
            "345/345 [==============================] - ETA: 0s - loss: 0.2284 - binary_accuracy: 0.7716WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 27/200\n",
            "324/345 [===========================>..] - ETA: 0s - loss: 0.2285 - binary_accuracy: 0.7715WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 28/200\n",
            "325/345 [===========================>..] - ETA: 0s - loss: 0.2301 - binary_accuracy: 0.7699WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 29/200\n",
            "341/345 [============================>.] - ETA: 0s - loss: 0.2274 - binary_accuracy: 0.7726WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 30/200\n",
            "340/345 [============================>.] - ETA: 0s - loss: 0.2286 - binary_accuracy: 0.7714WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 31/200\n",
            "341/345 [============================>.] - ETA: 0s - loss: 0.2283 - binary_accuracy: 0.7717WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 32/200\n",
            "335/345 [============================>.] - ETA: 0s - loss: 0.2282 - binary_accuracy: 0.7718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 33/200\n",
            "326/345 [===========================>..] - ETA: 0s - loss: 0.2287 - binary_accuracy: 0.7713WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 34/200\n",
            "339/345 [============================>.] - ETA: 0s - loss: 0.2273 - binary_accuracy: 0.7727WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 35/200\n",
            "322/345 [===========================>..] - ETA: 0s - loss: 0.2298 - binary_accuracy: 0.7702WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 36/200\n",
            "337/345 [============================>.] - ETA: 0s - loss: 0.2289 - binary_accuracy: 0.7711WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 37/200\n",
            "329/345 [===========================>..] - ETA: 0s - loss: 0.2281 - binary_accuracy: 0.7719WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 38/200\n",
            "325/345 [===========================>..] - ETA: 0s - loss: 0.2299 - binary_accuracy: 0.7701WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 39/200\n",
            "344/345 [============================>.] - ETA: 0s - loss: 0.2282 - binary_accuracy: 0.7718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 40/200\n",
            "336/345 [============================>.] - ETA: 0s - loss: 0.2292 - binary_accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 41/200\n",
            "338/345 [============================>.] - ETA: 0s - loss: 0.2276 - binary_accuracy: 0.7724WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 42/200\n",
            "326/345 [===========================>..] - ETA: 0s - loss: 0.2266 - binary_accuracy: 0.7734WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 43/200\n",
            "343/345 [============================>.] - ETA: 0s - loss: 0.2282 - binary_accuracy: 0.7718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 44/200\n",
            "330/345 [===========================>..] - ETA: 0s - loss: 0.2297 - binary_accuracy: 0.7703WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 45/200\n",
            "325/345 [===========================>..] - ETA: 0s - loss: 0.2279 - binary_accuracy: 0.7721WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 46/200\n",
            "326/345 [===========================>..] - ETA: 0s - loss: 0.2292 - binary_accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 47/200\n",
            "327/345 [===========================>..] - ETA: 0s - loss: 0.2287 - binary_accuracy: 0.7713WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 48/200\n",
            "340/345 [============================>.] - ETA: 0s - loss: 0.2282 - binary_accuracy: 0.7718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 49/200\n",
            "327/345 [===========================>..] - ETA: 0s - loss: 0.2296 - binary_accuracy: 0.7704WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 50/200\n",
            "326/345 [===========================>..] - ETA: 0s - loss: 0.2267 - binary_accuracy: 0.7733WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 51/200\n",
            "342/345 [============================>.] - ETA: 0s - loss: 0.2282 - binary_accuracy: 0.7718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 52/200\n",
            "323/345 [===========================>..] - ETA: 0s - loss: 0.2272 - binary_accuracy: 0.7728WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 53/200\n",
            "321/345 [==========================>...] - ETA: 0s - loss: 0.2288 - binary_accuracy: 0.7712WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 54/200\n",
            "339/345 [============================>.] - ETA: 0s - loss: 0.2282 - binary_accuracy: 0.7718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 55/200\n",
            "325/345 [===========================>..] - ETA: 0s - loss: 0.2305 - binary_accuracy: 0.7695WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 56/200\n",
            "329/345 [===========================>..] - ETA: 0s - loss: 0.2291 - binary_accuracy: 0.7709WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "345/345 [==============================] - 1s 2ms/step - loss: 0.2284 - binary_accuracy: 0.7716\n",
            "Epoch 57/200\n",
            "237/345 [===================>..........] - ETA: 0s - loss: 0.2338 - binary_accuracy: 0.7662"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-c8cdfafb26c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiM_zEX6epou",
        "outputId": "2cac48a7-8512-4081-9ee1-6562b0ac84f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "RN.evaluate(df_X, df_y)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "345/345 [==============================] - 0s 991us/step - loss: 0.2284 - binary_accuracy: 0.7716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22840569913387299, 0.7715942859649658]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agO4cGTqKmpz"
      },
      "source": [
        "A seguir, a matriz de confusão:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdiMhkVyaCDS"
      },
      "source": [
        "def Mostra_ConfusionMatrix():\n",
        "    y_pred = RN.predict_classes(df_X)\n",
        "    mc = confusion_matrix(df_y, y_pred)\n",
        "    #sns.heatmap(mc,annot=True, annot_kws={\"size\": 10},fmt=\"d\")\n",
        "    sns.heatmap(mc/np.sum(mc), annot=True, annot_kws={\"size\": 10}, fmt='.2%', cmap='Blues')"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLIAXu7SN7pV",
        "outputId": "0cda548c-10c6-416d-f896-d4ab9bafc416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "Mostra_ConfusionMatrix()"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-190-55a3bb500d9c>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5zYHcGuMPZe"
      },
      "source": [
        "### 8. _Fine tuning_ da Rede Neural\n",
        "\n",
        "Para aumentar a acurácia da Rede Neural, sugiro aumentarmos o número de neurônios na _Hidden Layer_ e/ou aumentar o número de _Hidden Layers_.\n",
        "\n",
        "No entanto, obtivemos uma acurácia razoável com a Rede Neural _baseline_. Portanto, deixo como exercício para os alunos o desafio de melhorar a acurácia desta Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ISodOu-Kmp3"
      },
      "source": [
        "### 9. Fazer Predições com a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgdL1W4vUrN"
      },
      "source": [
        "[**Python**] - Comando:\n",
        "* RN.predict_classes(X_treinamento);\n",
        "* RN.predict_classes(X_teste)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qun1-vOKmp4"
      },
      "source": [
        "y_pred = RN.predict_classes(df_X)\n",
        "y_pred = y_pred.astype('bool')\n",
        "df_y_pred = pd.DataFrame(y_pred.reshape(-1,1))"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_49PKLtGvTfo"
      },
      "source": [
        "df_df_y = pd.DataFrame(df_y.reshape(-1,1))"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRQCD3vPvadF"
      },
      "source": [
        "df_XOR = pd.DataFrame((df_y ^ y_pred).reshape(-1,1))"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI3HQOwCtoln",
        "outputId": "b434ec96-07e1-4958-cfd0-bf62e8d61c8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "resul = pd.concat([df_y_pred,df_df_y,df_XOR],axis = 1)\n",
        "resul"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11028</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11029</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11030</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11031</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11032</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11033 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0      0      0\n",
              "0      False   True   True\n",
              "1      False  False  False\n",
              "2      False  False  False\n",
              "3      False  False  False\n",
              "4      False  False  False\n",
              "...      ...    ...    ...\n",
              "11028  False  False  False\n",
              "11029  False  False  False\n",
              "11030  False   True   True\n",
              "11031  False  False  False\n",
              "11032  False   True   True\n",
              "\n",
              "[11033 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOIfNJKEhUM3",
        "outputId": "caed06c3-439a-4a18-f825-6951d7c62477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dif =1 - df_XOR.sum()/len(df_y)\n",
        "dif"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.763981\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG1e5A-GffMd"
      },
      "source": [
        "df_test = pd.read_csv('/test_2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jdL1-azgE1W"
      },
      "source": [
        "X_teste = df_test.drop(columns = ['id'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1mD3H9Gf5PH",
        "outputId": "90bd37c8-4937-42e7-a95b-c2c743350379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cnae2</th>\n",
              "      <th>rf2</th>\n",
              "      <th>md1</th>\n",
              "      <th>md2</th>\n",
              "      <th>md3</th>\n",
              "      <th>md4</th>\n",
              "      <th>md5</th>\n",
              "      <th>md6</th>\n",
              "      <th>md7</th>\n",
              "      <th>md8</th>\n",
              "      <th>md9</th>\n",
              "      <th>md10</th>\n",
              "      <th>md11</th>\n",
              "      <th>md12</th>\n",
              "      <th>mc1</th>\n",
              "      <th>mc2</th>\n",
              "      <th>mc3</th>\n",
              "      <th>mc4</th>\n",
              "      <th>ind01</th>\n",
              "      <th>ind02</th>\n",
              "      <th>ind03</th>\n",
              "      <th>ind04</th>\n",
              "      <th>ind05</th>\n",
              "      <th>ind06</th>\n",
              "      <th>ind07</th>\n",
              "      <th>ind08</th>\n",
              "      <th>ind09</th>\n",
              "      <th>ind10</th>\n",
              "      <th>ind11</th>\n",
              "      <th>ind12</th>\n",
              "      <th>ind13</th>\n",
              "      <th>ind14</th>\n",
              "      <th>ind15</th>\n",
              "      <th>ind16</th>\n",
              "      <th>ind17</th>\n",
              "      <th>ind18</th>\n",
              "      <th>ind19</th>\n",
              "      <th>ind20</th>\n",
              "      <th>ind21</th>\n",
              "      <th>ind22</th>\n",
              "      <th>ind23</th>\n",
              "      <th>ind24</th>\n",
              "      <th>ind25</th>\n",
              "      <th>ind26</th>\n",
              "      <th>ind27</th>\n",
              "      <th>ind28</th>\n",
              "      <th>ind29</th>\n",
              "      <th>ind30</th>\n",
              "      <th>ind31</th>\n",
              "      <th>ind32</th>\n",
              "      <th>ind33</th>\n",
              "      <th>ind34</th>\n",
              "      <th>ind35</th>\n",
              "      <th>ind36</th>\n",
              "      <th>ind37</th>\n",
              "      <th>ind38</th>\n",
              "      <th>ind39</th>\n",
              "      <th>ind40</th>\n",
              "      <th>ind41</th>\n",
              "      <th>ind42</th>\n",
              "      <th>ind43</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3411</td>\n",
              "      <td>71</td>\n",
              "      <td>6</td>\n",
              "      <td>0.015101</td>\n",
              "      <td>0.004743</td>\n",
              "      <td>0.111771</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>0.003121</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005432</td>\n",
              "      <td>0.023085</td>\n",
              "      <td>0.009890</td>\n",
              "      <td>0.011346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131320</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.442161e-09</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0326</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2177</td>\n",
              "      <td>86</td>\n",
              "      <td>8</td>\n",
              "      <td>0.012269</td>\n",
              "      <td>0.005919</td>\n",
              "      <td>0.111803</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006160</td>\n",
              "      <td>0.022035</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132734</td>\n",
              "      <td>0.003945</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.343705e-06</td>\n",
              "      <td>0.001462</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.36075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8400</td>\n",
              "      <td>41</td>\n",
              "      <td>9</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>0.109870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002823</td>\n",
              "      <td>0.020522</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131390</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.473043e-05</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>464</td>\n",
              "      <td>58</td>\n",
              "      <td>9</td>\n",
              "      <td>0.015101</td>\n",
              "      <td>0.028263</td>\n",
              "      <td>0.129650</td>\n",
              "      <td>0.028248</td>\n",
              "      <td>0.005284</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036084</td>\n",
              "      <td>0.050898</td>\n",
              "      <td>0.040644</td>\n",
              "      <td>0.004436</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.138620</td>\n",
              "      <td>0.003389</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.104422e-03</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4810</td>\n",
              "      <td>0.8654</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.19260</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8694</td>\n",
              "      <td>0.9212</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1785</td>\n",
              "      <td>0.1389</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6672</td>\n",
              "      <td>86</td>\n",
              "      <td>3</td>\n",
              "      <td>0.011190</td>\n",
              "      <td>0.004536</td>\n",
              "      <td>0.110678</td>\n",
              "      <td>0.002873</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004536</td>\n",
              "      <td>0.020658</td>\n",
              "      <td>0.003853</td>\n",
              "      <td>0.004202</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.130692</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0153</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7799</td>\n",
              "      <td>0.7799</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  cnae2  rf2       md1       md2  ...  ind39  ind40  ind41  ind42  ind43\n",
              "0  3411     71    6  0.015101  0.004743  ...    0.0    0.0    0.0    0.0    0.0\n",
              "1  2177     86    8  0.012269  0.005919  ...    0.0    0.0    0.0    0.0    0.0\n",
              "2  8400     41    9  0.002325  0.001882  ...    0.0    0.0    0.0    0.0    0.0\n",
              "3   464     58    9  0.015101  0.028263  ...    0.0    0.0    0.0    0.0    0.0\n",
              "4  6672     86    3  0.011190  0.004536  ...    0.0    0.0    0.0    0.0    0.0\n",
              "\n",
              "[5 rows x 62 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqy2rt3YfPiy"
      },
      "source": [
        "y_teste = RN.predict_classes(X_teste)\n",
        "\n",
        "df_submit = pd.DataFrame(zip(df_test['id'],y_teste), columns = ['id','target'])\n",
        "\n",
        "df_submit.to_csv('/PyLadies_NL.csv',index = False, sep = ',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C_u02mygKgt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU_6XlFRgPL2",
        "outputId": "9d84cec9-19c7-4b1f-c5c7-feeb9c4f70a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_submit['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    794\n",
              "1    206\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvywP0nZMtA-"
      },
      "source": [
        "### 10. Conclusões\n",
        "\n",
        "Desenvolvemos uma Rede Neural capaz de identificar Sexo (_Gender_) com acurácia= 0.9120."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL2g2pn-RfJi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}